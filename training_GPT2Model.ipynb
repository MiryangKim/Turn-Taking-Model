{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43304ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall tensorflow -y\n",
    "#!pip  install transformers==4.22.1 -q\n",
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2b3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2Tokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, \\\n",
    "                         GPT2ForSequenceClassification, set_seed, pipeline,GPT2Config\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f433668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "from load_data import Data\n",
    "import train_datasets\n",
    "from train_datasets import Train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf36063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    #Load a datafraom from the Data class from load_data\n",
    "    data.handle_file()\n",
    "    data.convert_json_to_dataframe()\n",
    "    data.get_next_value()\n",
    "    data.compare_values()\n",
    "    data.label_sentences()\n",
    "    data.initial_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25ac409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    #Mapping tokenizer with a dataset\n",
    "    return tokenizer(dataset[\"text\"], truncation=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fd80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):    \n",
    "    #Setting evaluation metrics\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a57cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model():\n",
    "    #Defining Model and training arguments \n",
    "    #Training model with splitted dataset\n",
    "    uni_labels = ['Continue', \"shift\"]\n",
    "    gpt2_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "    gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "    gpt2_model.config.id2label = {i: l for i, l in enumerate(uni_labels)}\n",
    "    gpt2_model.config.pad_token_id = gpt2_model.config.eos_token_id\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results_gpt',\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.05,\n",
    "        logging_dir='./logs_gpt',\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=1,\n",
    "        log_level='info',\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=100,\n",
    "        save_strategy='epoch'\n",
    "    )\n",
    "\n",
    "    trainer1 = Trainer(\n",
    "        model=gpt2_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test1'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer2 = Trainer(\n",
    "        model=gpt2_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test2'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    return trainer1, trainer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a6d149",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b81730200d14dfc9aba7f4a7f483e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb67d4cbc0d45d28e31eadc580224fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e0772cb02b4366bf821c19742ef102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\singg\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10834\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5418' max='5418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5418/5418 2:03:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138604</td>\n",
       "      <td>0.957906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152401</td>\n",
       "      <td>0.960122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3611\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_gpt\\checkpoint-2709\n",
      "Configuration saved in ./results_gpt\\checkpoint-2709\\config.json\n",
      "Model weights saved in ./results_gpt\\checkpoint-2709\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3611\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_gpt\\checkpoint-5418\n",
      "Configuration saved in ./results_gpt\\checkpoint-5418\\config.json\n",
      "Model weights saved in ./results_gpt\\checkpoint-5418\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_gpt\\checkpoint-2709 (score: 0.13860446214675903).\n",
      "Saving model checkpoint to ./model/bert_distilbert_seq_token\n",
      "Configuration saved in ./model/bert_distilbert_seq_token\\config.json\n",
      "Model weights saved in ./model/bert_distilbert_seq_token\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #============================================\n",
    "    #Generate a df from Data class from data_load\n",
    "    #A df contains \"texc\" column a sentence per a row and their lables(0: continue, 1: change)\n",
    "    path ='hotels.json'\n",
    "    data = Data(path)\n",
    "    load_dataset()\n",
    "\n",
    "    #save initial df as a json file\n",
    "    df = data.df.to_json(\"df.json\")\n",
    "\n",
    "    #============================================\n",
    "    #Preprocessing data for modeling\n",
    "    t = data.df.copy()\n",
    "    t = t[:4000]\n",
    "    #4000 rows were used which will generate sequences.    \n",
    "\n",
    "    #Splitting setences to tokens and labeling tokens\n",
    "    train_dataset = Train_dataset(t) \n",
    "    train_dataset.test_text = t['text'].values\n",
    "    train_dataset.test_label = t['label'].values\n",
    "    \n",
    "    train_dataset.split_token_sentences()\n",
    "    train_dataset.tokenized_text_label = train_dataset.flatten_list(train_dataset.tokenized_text_label)\n",
    "    train_dataset.tokenized_text = train_dataset.flatten_list(train_dataset.tokenized_text)\n",
    "    \n",
    "    #Generating sequnces to be used for training by combining tokens and labeling the sequences\n",
    "    train_dataset.generate_test_dataset()\n",
    "    \n",
    "    #Generating datasets for training, vaildating and testing\n",
    "    train_dataset.datasets_for_training()\n",
    "    \n",
    "    #Applying DistilBertTokenizerFast for DistilBert model\n",
    "    dataset = train_dataset.dataset\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    dataset = dataset.remove_columns(['text'])\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "    \n",
    "    #padding dataset\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "    \n",
    "    #training and saving models\n",
    "    trainer1, trainer2 = training_model()\n",
    "    \n",
    "    #trainer1.evaluate()\n",
    "    #tarining modesl with preprocessed dataset\n",
    "    trainer1.train()\n",
    "    #evaluation trained model\n",
    "    #trainer1.evaluate()\n",
    "    #evaluation model with another dataset\n",
    "    #trainer2.evaluate()\n",
    "    trainer1.save_model('./model/bert_distilbert_seq_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44b049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
